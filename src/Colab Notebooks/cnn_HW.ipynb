{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn_HW.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"MoxiyFnkOFFT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"0d8b9c7d-b076-402c-dcd6-4d708f55a455","executionInfo":{"status":"ok","timestamp":1542603563073,"user_tz":-60,"elapsed":2958,"user":{"displayName":"SeongHyeon Park","photoUrl":"","userId":"02263789689507850061"}}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","from tensorflow.keras.datasets.cifar10 import load_data\n","\n","tf.set_random_seed(777)  # reproducibility\n","\n","print (\"!!!Your Goal is to Get at least 90% of Accuracy!!!\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","!!!Your Goal is to Get at least 90% of Accuracy!!!\n"],"name":"stdout"}]},{"metadata":{"id":"5TTID0oXRyTA","colab_type":"code","colab":{}},"cell_type":"code","source":["# you can change hyperparameters if you want\n","####################################\n","###           HOMEWORK           ###\n","#################################### \n","learning_rate = 1e-3\n","dropout_rate = 0.7\n","batch_size = 128\n","epoch = 1000"],"execution_count":0,"outputs":[]},{"metadata":{"id":"naKSDk6WOLoo","colab_type":"code","colab":{}},"cell_type":"code","source":["# define your CNN model!\n","# you can use ensemble if you want.\n","####################################\n","###           HOMEWORK           ###\n","####################################\n","def cnn_custom(x): \n","  # input CIFAR image size : 32 x 32 x 3 \"RGB\"\n","  x_image = x\n","  \n","  '''\n","  Do implementation.\n","  pls check that return = y_pred, logits\n","  '''\n","  \n","  \n","  y_pred = tf.nn.softmax(logits)\n","  return y_pred, logits"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RcUQ4tetQejR","colab_type":"code","colab":{}},"cell_type":"code","source":["# CIFAR-10 data load\n","(x_train, y_train), (x_test, y_test) = load_data()\n","\n","# scalar label (0~9) -> One-hot Encoding \n","y_train_one_hot = tf.squeeze(tf.one_hot(y_train, 10),axis=1)\n","y_test_one_hot = tf.squeeze(tf.one_hot(y_test, 10),axis=1)\n","\n","# placeholder\n","x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n","y = tf.placeholder(tf.float32, shape=[None, 10])\n","keep_prob = tf.placeholder(tf.float32)\n","\n","# build CNN graphs\n","y_pred, logits = cnn_custom(x)\n","\n","# loss func & optimizer\n","loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits))\n","train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n","\n","# accuracy\n","correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HjGVBnpJQPFa","colab_type":"code","colab":{}},"cell_type":"code","source":["def next_batch(num, data, labels):\n","  '''\n","  return data & labels of num -> you don't have to care this function\n","  '''\n","  idx = np.arange(0 , len(data))\n","  np.random.shuffle(idx)\n","  idx = idx[:num]\n","  data_shuffle = [data[ i] for i in idx]\n","  labels_shuffle = [labels[ i] for i in idx]\n","\n","  return np.asarray(data_shuffle), np.asarray(labels_shuffle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t9fj-i59QSvm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":382},"outputId":"1b5e72ac-f6a3-403d-a387-60302216c492"},"cell_type":"code","source":["with tf.Session() as sess: \n","  sess.run(tf.global_variables_initializer())\n","  \n","  for i in range(epoch):\n","    batch = next_batch(batch_size, x_train, y_train_one_hot.eval())\n","\n","    # print accuracy & loss every 100 Steps\n","    if (i+1) % 100 == 0:\n","      train_accuracy = accuracy.eval(feed_dict={x: batch[0], y: batch[1], keep_prob: 1.0})\n","      loss_print = loss.eval(feed_dict={x: batch[0], y: batch[1], keep_prob: 1.0})\n","\n","      print(\"Epoch: %d, Accuracy: %f, loss: %f\" % (i+1, train_accuracy, loss_print))\n","    # dropout rate\n","    sess.run(train_step, feed_dict={x: batch[0], y: batch[1], keep_prob: dropout_rate})\n","\n","  # test data accuracy \n","  test_accuracy = 0.0  \n","  for i in range(10):\n","    test_batch = next_batch(1000, x_test, y_test_one_hot.eval())\n","    test_accuracy = test_accuracy + accuracy.eval(feed_dict={x: test_batch[0], y: test_batch[1], keep_prob: 1.0})\n","  test_accuracy = test_accuracy / 10;\n","print(\"Test data Accuracy: %f\" % test_accuracy)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 100, Accuracy: 0.343750, loss: 1.670681\n","Epoch: 200, Accuracy: 0.453125, loss: 1.509658\n","Epoch: 300, Accuracy: 0.476562, loss: 1.480438\n","Epoch: 400, Accuracy: 0.507812, loss: 1.477707\n","Epoch: 500, Accuracy: 0.507812, loss: 1.319416\n","Epoch: 600, Accuracy: 0.609375, loss: 1.246467\n","Epoch: 700, Accuracy: 0.593750, loss: 1.251301\n","Epoch: 800, Accuracy: 0.609375, loss: 1.220586\n","Epoch: 900, Accuracy: 0.617188, loss: 1.133587\n","Epoch: 1000, Accuracy: 0.671875, loss: 0.994839\n","Epoch: 1100, Accuracy: 0.710938, loss: 0.928988\n","Epoch: 1200, Accuracy: 0.625000, loss: 1.068607\n","Epoch: 1300, Accuracy: 0.640625, loss: 1.047035\n","Epoch: 1400, Accuracy: 0.718750, loss: 0.873042\n","Epoch: 1500, Accuracy: 0.710938, loss: 0.910079\n","Epoch: 1600, Accuracy: 0.687500, loss: 0.921457\n","Epoch: 1700, Accuracy: 0.671875, loss: 0.987435\n","Epoch: 1800, Accuracy: 0.640625, loss: 0.960387\n","Epoch: 1900, Accuracy: 0.726562, loss: 0.812195\n","Epoch: 2000, Accuracy: 0.695312, loss: 0.779115\n","Epoch: 2100, Accuracy: 0.687500, loss: 0.765573\n"],"name":"stdout"}]}]}